{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2025-01-19'\n",
    "data = pd.read_csv('../data/oresund/'+date+'.csv')\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ships:  379 \n",
      "Number of data points:  616606\n",
      "Index(['# Timestamp', 'Type of mobile', 'MMSI', 'Latitude', 'Longitude',\n",
      "       'Navigational status', 'SOG', 'COG', 'Heading', 'Ship type',\n",
      "       'Cargo type', 'Width', 'Length', 'Destination'],\n",
      "      dtype='object')\n",
      "['Undefined' 'Cargo' 'Other' 'Passenger' 'Pleasure' 'Sailing'\n",
      " 'Port tender' 'Pilot' 'Dredging' 'SAR' 'Tug' 'Military' 'Towing' 'Tanker'\n",
      " 'Diving' 'HSC' 'Reserved' 'Fishing' 'Law enforcement']\n",
      "['Under way using engine' 'Unknown value' 'Moored'\n",
      " 'Restricted maneuverability' 'At anchor' 'Not under command'\n",
      " 'Power-driven vessel pushing ahead or towing alongside']\n",
      "0    19/01/2025 00:00:00\n",
      "1    19/01/2025 00:00:00\n",
      "2    19/01/2025 00:00:00\n",
      "6    19/01/2025 00:00:00\n",
      "7    19/01/2025 00:00:00\n",
      "Name: # Timestamp, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Getting a quick overview of the data\n",
    "nrships = len(data['MMSI'].unique())\n",
    "lendata = len(data)\n",
    "\n",
    "print('Number of ships: ', nrships ,\n",
    "      '\\nNumber of data points: ', lendata)\n",
    "\n",
    "print(data.columns)\n",
    "\n",
    "print(data['Ship type'].unique())\n",
    "\n",
    "print(data['Navigational status'].unique()) #I don't really want to filter on dynamical statuses\n",
    "#because they can be false...\n",
    "\n",
    "print(data['# Timestamp'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/29/jqsnm5js6_sc7663vjsq3mp40000gp/T/ipykernel_40760/1930749798.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data_temp = pd.concat([data_temp, ship], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "data_filter = data.copy()\n",
    "\n",
    "#I filter for helicopter\n",
    "data_filter = data_filter[(data_filter['SOG'] < 50) | (data_filter['SOG'].isna())] #A helicopter is faster than 50 knots and some ships are noobs\n",
    "print(len(data_filter['MMSI'].unique()))\n",
    "\n",
    "data_temp = pd.DataFrame(columns=data_filter.columns)\n",
    "#I need to filter down the time if things are to be scalable\n",
    "#This does it one a five minute basis\n",
    "for MMSI in data_filter['MMSI'].unique():\n",
    "    ship = data_filter[data_filter['MMSI'] == MMSI]\n",
    "    #Creating a series for time filtering\n",
    "    series = pd.Series(range(len(ship)), index=pd.to_datetime(ship['# Timestamp'], dayfirst=True))\n",
    "    #Resampling the series to 5 minutes. This gives the index of the last data point in the 5 minute interval\n",
    "    #The unique is added because if transmission occurs less frequently than 5 mins there will be duplicates\n",
    "    resampled = (series.resample('5min').count().cumsum()-1).unique()\n",
    "    #Filtering the data\n",
    "    ship = ship.iloc[resampled]\n",
    "    data_temp = pd.concat([data_temp, ship], ignore_index=True)\n",
    "\n",
    "data_filter = data_temp.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the filtered data\n",
    "date = '2025-01-19'\n",
    "filename = '../data/oresund/'+date+'_filtered.csv'\n",
    "if not os.path.exists(filename):\n",
    "    data_filter.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "down here is me fooling around to learn the relevant commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         2025-01-19 00:00:00\n",
      "1         2025-01-19 00:00:00\n",
      "2         2025-01-19 00:00:00\n",
      "6         2025-01-19 00:00:00\n",
      "7         2025-01-19 00:00:00\n",
      "                  ...        \n",
      "1936779   2025-01-19 23:59:58\n",
      "1936780   2025-01-19 23:59:58\n",
      "1936786   2025-01-19 23:59:58\n",
      "1936788   2025-01-19 23:59:58\n",
      "1936793   2025-01-19 23:59:58\n",
      "Name: # Timestamp, Length: 615750, dtype: datetime64[ns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/29/jqsnm5js6_sc7663vjsq3mp40000gp/T/ipykernel_40760/3133416223.py:2: UserWarning: Parsing dates in %d/%m/%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  dt = pd.to_datetime(data_filter['# Timestamp'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23.999444444444446"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is me fooling around with timestamps\n",
    "dt = pd.to_datetime(data_filter['# Timestamp'])\n",
    "print(dt)\n",
    "(dt.iloc[-1] - dt.iloc[0]).total_seconds()/(60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.884027777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/29/jqsnm5js6_sc7663vjsq3mp40000gp/T/ipykernel_40760/3016659332.py:4: UserWarning: Parsing dates in %d/%m/%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  ship_time = pd.to_datetime(ship['# Timestamp'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n",
       "       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n",
       "       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
       "       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
       "       144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
       "       157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "       170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
       "       183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "       196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
       "       209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221,\n",
       "       222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234,\n",
       "       235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
       "       248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260,\n",
       "       261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273,\n",
       "       274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286,\n",
       "       287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299,\n",
       "       300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312,\n",
       "       313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
       "       326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338,\n",
       "       339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351,\n",
       "       352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364,\n",
       "       365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
       "       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390,\n",
       "       391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403,\n",
       "       404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416,\n",
       "       417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429,\n",
       "       430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442,\n",
       "       443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455,\n",
       "       456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468,\n",
       "       469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fooling around with MMSI number\n",
    "MMSI_ar = data_filter['MMSI'].unique()\n",
    "ship = data_filter[data_filter['MMSI'] == MMSI_ar[0]]\n",
    "ship_time = pd.to_datetime(ship['# Timestamp'])\n",
    "ship_time.index = ship_time\n",
    "new_ship_time = ship_time.iloc[ship_time.resample('1min').count().cumsum()-1]\n",
    "\n",
    "print(len(ship_time)/len(new_ship_time))\n",
    "\n",
    "#How about if it does not signal every minute?\n",
    "index = pd.date_range(start=ship_time.index[0], end=ship_time.index[-1], freq='3min')\n",
    "series = pd.Series(range(len(index)), index=index)\n",
    "series.resample('1min').count().cumsum().unique()\n",
    "\n",
    "#I need the unique part because the cumsum will give me the same value for all the 3 minute intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding new stuff 2/3 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Class A' 'Class B' 'Base Station' 'AtoN'] 69149\n",
      "['Class A' 'Class B'] 67423\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/oresund/2025-01-19_filtered.csv')\n",
    "print(data['Type of mobile'].unique(), len(data))\n",
    "data = data[(data['Type of mobile']=='Class A') | (data['Type of mobile']=='Class B')]\n",
    "print(data['Type of mobile'].unique(), len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving data 2/3 2025\n",
    "date = '2025-01-19'\n",
    "filename = '../data/oresund/'+date+'_filtered_2.csv'\n",
    "if not os.path.exists(filename):\n",
    "    data.to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
