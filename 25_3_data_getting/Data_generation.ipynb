{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this file is to download and filter a years (or all the data that is available in single days from AIS webpage) worth of data!\n",
    "Loop over all files and do:\n",
    "1) Download \n",
    "2) Unzip\n",
    "3) Filter Oresund region\n",
    "4) Sample in 5 mins intervals\n",
    "5) Do additional filtering steps\n",
    "6) Save in manner such that single ships are saved in single files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download a specific chunk\n",
    "def download_chunk(start, end, chunk_idx, progress_bar, url, fname):\n",
    "    headers = {\"Range\": f\"bytes={start}-{end}\"}\n",
    "    chunk_fname = f\"{fname}.part{chunk_idx}\"\n",
    "    \n",
    "    #Stream=True to download in chunks and avoid loading the entire file into memory\n",
    "    with requests.get(url, headers=headers, stream=True) as r, open(chunk_fname, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024 * 1024 * 10):  # 10 MB chunks\n",
    "            f.write(chunk)\n",
    "            progress_bar.update(len(chunk))\n",
    "\n",
    "    return chunk_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://web.ais.dk/aisdata/aisdk-2024-04-01.zip (552.159067 MB) in 4 chunks of size 138.039766 MB each.\n"
     ]
    }
   ],
   "source": [
    "url_home = \"https://web.ais.dk/aisdata/\" #Initializing URL \n",
    "date_0 = pd.Timestamp('2024-04-01') #Start date of download\n",
    "date_end = pd.Timestamp('2025-03-28') #End date of download\n",
    "data_home = \"../Data/25_3_data_getting/data_generation/\" #Local directory to save data\n",
    "\n",
    "#If donwload is interrupted or needs to be resumed/extended, make a set of existing ships\n",
    "existing_ships = set()\n",
    "for ship in os.listdir(data_home):\n",
    "    existing_ships.add(int(ship.split('.')[0]))\n",
    "\n",
    "num_chunks = 4  # Number of parallel chunks (adjust based on network speed) while downloading\n",
    "\n",
    "while date_0 < date_end: #Download until the end date\n",
    "    url = url_home + \"aisdk-\" + date_0.strftime(\"%Y-%m-%d\") + \".zip\" #URL for the specific date\n",
    "    fname = data_home + \"aisdk-\" + date_0.strftime(\"%Y-%m-%d\") + \".zip\" #Local file name\n",
    "    response = requests.head(url) #Get information about the file\n",
    "    file_size = int(response.headers.get(\"Content-Length\", 0)) #File size in bytes\n",
    "    chunk_size = file_size // num_chunks #Size of each chunk\n",
    "    print(f\"Downloading {url} ({file_size/10**6} MB) in {num_chunks} chunks of size {chunk_size/10**6} MB each.\")\n",
    "\n",
    "    chunk_ranges = [(i * chunk_size, (i + 1) * chunk_size - 1, i) for i in range(num_chunks)] #Creating ranges for each chunk (start, end, chunk_idx)\n",
    "    chunk_ranges[-1] = (chunk_ranges[-1][0], file_size - 1, num_chunks - 1)  # Ensure last chunk gets remaining bytes\n",
    "\n",
    "    #Downloading the file in parallel using ThreadPoolExecutor\n",
    "    #Using tqdm to show progress bar for the entire download\n",
    "    with tqdm(total=file_size, unit=\"B\", unit_scale=True, desc=\"Downloading\") as progress_bar:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_chunks) as executor:\n",
    "            chunk_files = list(executor.map(lambda args: download_chunk(*args, progress_bar, url, fname), chunk_ranges))\n",
    "\n",
    "    #Merging the downloaded chunks into a single file\n",
    "    #Using a context manager to handle file operations\n",
    "    #Deleting temporary chunk files after merging\n",
    "    with open(fname, \"wb\") as output_file:\n",
    "        for chunk_file in chunk_files:\n",
    "            with open(chunk_file, \"rb\") as part:\n",
    "                output_file.write(part.read())\n",
    "            os.remove(chunk_file)  # Delete temporary chunk files\n",
    "\n",
    "\n",
    "    with zipfile.ZipFile(fname, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_home)\n",
    "        print(\"Files extracted\")\n",
    "        #Deleting the zip file after extraction\n",
    "    os.remove(fname)  # Delete the zip file after extraction\n",
    "\n",
    "    #Now onto the csv filtering!\n",
    "    fname = data_home + \"aisdk-\" + date_0.strftime(\"%Y-%m-%d\") + \".csv\" #Local file name for the extracted CSV file\n",
    "\n",
    "    date_0 += pd.DateOffset(days=1) #Increment date by one day\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO:\n",
    "Think hard about how it is good to store data like this (for using for real later)\n",
    "Is it one single large file?\n",
    "Is it shipwise?\n",
    "Is it datewise?\n",
    "Is it both ship and datewise???\n",
    "\n",
    "I will do many computations for a single ship but in the end want to fx plot by days\n",
    "\n",
    "Take inspiration in what we did in ML project???\n",
    "\n",
    "Find more efficient way to merge parquet or csv files! (inspiration from chunk download!)\n",
    "\n",
    "The data will still be very large with a whole year (Gigs)\n",
    "Find out how to separate static data from ships out! And maybe store in a different setting! (Pretty easy to postpone if I save as parquet I think!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"file1.csv\", \"a\") as f1, open(\"file2.csv\", \"r\") as f2:\n",
    "#     next(f2)  # Skip header in second file\n",
    "#     for line in f2:\n",
    "#         f1.write(line)\n",
    "\n",
    "\n",
    "# import dask.dataframe as dd\n",
    "\n",
    "# df1 = dd.read_csv(\"file1.csv\")\n",
    "# df2 = dd.read_csv(\"file2.csv\")\n",
    "\n",
    "# merged = dd.concat([df1, df2])  # Efficient, lazy merge\n",
    "# merged.to_csv(\"merged_dask.csv\", single_file=True, index=False)  # Write back as a single CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-01 00:00:00\n",
      "2024-04-30 00:00:00\n",
      "2024-05-01\n"
     ]
    }
   ],
   "source": [
    "t = pd.Timestamp('2024-04-01')\n",
    "print(t)\n",
    "t += pd.DateOffset(days=29)\n",
    "print(t)\n",
    "t += pd.DateOffset(days=1)\n",
    "print(t.__str__().split(' ')[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
